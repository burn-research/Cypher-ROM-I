{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc0tw7b3YLfx"
      },
      "source": [
        "# Hands-on session on reduced-order models I\n",
        "\n",
        "In this session, we are going to have a look at how we can use the Proper Orthogonal Decomposition (POD) and the Principal Component Analysis (PCA) to analyse a combustion dataset.\n",
        "\n",
        "As we'll see, POD and PCA represent the same mathematical technique but they are employed in slightly different ways. POD is generally used for the analysis of time-evolving reacting flows while PCA is used to find a low-dimensional representation of the thermo-chemical space (the manifold).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGDjLYkd7JcK"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset comes from the 2D simulation of a pulsating $\\mathrm{CH}_4$/$\\mathrm{N}_2$ laminar flame. The pulsating behaviour is achieved by modulating the inlet velocity with two different characteristic frequencies:\n",
        "* simple 10 Hz modulation,\n",
        "* complex 10, 20, 80 Hz modulation.\n",
        "\n",
        "The grid contains $n_c = 21134$ cells, and it is stored in the file \"grid.vtu\".\n",
        "\n",
        "The dataset contains 3 files. In the \"pod\" files, the two simulations are arranged in the POD way (the columns contain different timesteps). Each file contains a matrix of size $n \\times m$, where $n = n_f n_c = 2 \\cdot 21134$. The two features included are the temperature in K and the mass fraction of $\\mathrm{OH}$, which is a radical species used to identify the reaction zone.\n",
        "The number of timesteps is $m = 516$, with a $\\Delta t = 5\\cdot 10^{-4}$ s.\n",
        "\n",
        "In the \"pca\" file, the data of the two simulations is arranged in a single matrix of size $2 n_c m \\times n_f = 10820608 \\times 9$. The number of timesteps included is $256$ and the features included are the temperature and the species mass fraction of $\\mathrm{CH}4$, $\\mathrm{CO}$, $\\mathrm{CO}_2$, $\\mathrm{H}_2$, $\\mathrm{H}_2\\mathrm{O}$, $\\mathrm{N}_2$, $\\mathrm{O}_2$, $\\mathrm{OH}$\n",
        "\n",
        "The dataset has been uploaded on zenodo and it can be found here https://zenodo.org/doi/10.5281/zenodo.13490193. We can dowload it directly using ```zenodo_get```, as will see in the next cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7wDI7mHNXTK2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# We install some packages that we'll need later\n",
        "! pip install zenodo_get\n",
        "! pip install pyvista\n",
        "! pip install openmeasure\n",
        "! git clone http://gitlab.multiscale.utah.edu/common/PCAfold.git\n",
        "%cd PCAfold\n",
        "! python -m pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BH05sXfVur-",
        "outputId": "8a17e3c1-d256-45b6-ad6e-8c2c150dda8d"
      },
      "outputs": [],
      "source": [
        "import zenodo_get\n",
        "! zenodo_get -d 10.5281/zenodo.13490193"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "OCEtSFl-zYZa",
        "outputId": "b797c9fd-8708-49a8-cec0-d0b970a84d40"
      },
      "outputs": [],
      "source": [
        "# Visualise the dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import pyvista as pv\n",
        "import numpy as np\n",
        "\n",
        "# this function can be used to plot a single snapshot\n",
        "def plot_snapshot(mesh, z, normal, origin, axis, feature, cmap='viridis', filename=''):\n",
        "    mesh['z'] = z\n",
        "\n",
        "    plane = mesh.ctp().slice(normal=normal, origin=origin, generate_triangles=True)\n",
        "\n",
        "    vmin = np.min(plane['z'].min())\n",
        "    vmax = np.max(plane['z'].max())\n",
        "\n",
        "    x = plane.points*1000\n",
        "    tri = plane.faces.reshape((-1,4))[:, 1:]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(2.3, 4))\n",
        "    levels = 64\n",
        "\n",
        "    axis_labels = ['x', 'y', 'z']\n",
        "    fig.subplots_adjust(bottom=0.1, top=.9, left=0.1, right=0.8)\n",
        "\n",
        "    cs = ax.tricontourf(x[:,axis[0]], x[:,axis[1]], tri, plane['z'], cmap=cmap,\n",
        "                    levels=levels, vmin=vmin, vmax=vmax)\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel(f'{axis_labels[axis[0]]} (mm)')\n",
        "    ax.set_ylabel(f'{axis_labels[axis[1]]} (mm)')\n",
        "\n",
        "\n",
        "    ax_bounds = ax.get_position().bounds\n",
        "    cb_ax = fig.add_axes([0.825, ax_bounds[1], 0.05, ax_bounds[3]])\n",
        "    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
        "    cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
        "                        cax=cb_ax, orientation='vertical', label=feature)\n",
        "    fmt = mpl.ticker.ScalarFormatter(useMathText=True)\n",
        "    cbar.formatter.set_powerlimits((0, 4))\n",
        "    cb_ax.yaxis.set_offset_position('left')\n",
        "\n",
        "    if filename != '':\n",
        "        fig.savefig(filename, transparent=False, dpi=600, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "grid = pv.read('grid.vtu') # Load the computational grid\n",
        "\n",
        "xyz = grid.cell_centers().points\n",
        "n_cells = xyz.shape[0] # Number of cells\n",
        "\n",
        "features_pod = ['T', 'OH']\n",
        "\n",
        "# Select the test case\n",
        "test_case = 'f10'\n",
        "D = np.load(f'D_pod_{test_case}.npy')\n",
        "m = D.shape[1]\n",
        "\n",
        "# Plot a single snapshot\n",
        "f_plot = 'T'\n",
        "i_plot = features_pod.index(f_plot)\n",
        "snapshot = D[i_plot*n_cells:(i_plot+1)*n_cells, 0]\n",
        "\n",
        "plot_snapshot(grid, snapshot, (0,1,0), (0,0,0), (0,2),\n",
        "              features_pod[i_plot], cmap='inferno')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "NgHDR6yPGwiE",
        "outputId": "a239dcac-8cca-42cb-d47b-dbb916ce4abf"
      },
      "outputs": [],
      "source": [
        "# We can plot an animation to show the temporal behaviour of the dataset\n",
        "\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "\n",
        "def animate(i):\n",
        "    im1.set_array(D_samples[:n_cells_s,i].reshape(sample_res))\n",
        "    im2.set_array(D_samples[n_cells_s:,i].reshape(sample_res))\n",
        "    print(f'Frame {i+1}/{n_frames}', flush=True, end='\\r')\n",
        "\n",
        "    return im1, im2\n",
        "\n",
        "f_plot = ['T', 'OH']\n",
        "i_plot = []\n",
        "for f in f_plot:\n",
        "    i_plot.append(features_pod.index(f))\n",
        "\n",
        "gif_step = 5\n",
        "for i in i_plot:\n",
        "    for j in range(m//gif_step):\n",
        "        temp = D[i*n_cells:(i+1)*n_cells, j*gif_step]\n",
        "        grid[f'f_{i}_timestep_{j}'] = temp\n",
        "\n",
        "dx = 5e-4\n",
        "D_samples = np.arange(0, 0.025, dx)\n",
        "z_samples = np.arange(0, 0.1, dx)\n",
        "\n",
        "D_samples_grid, z_samples_grid = np.meshgrid(D_samples, z_samples)\n",
        "xyz_samples = np.zeros((D_samples_grid.size, 3))\n",
        "xyz_samples[:,0] = D_samples_grid.flatten()\n",
        "xyz_samples[:,2] = z_samples_grid.flatten()\n",
        "n_cells_s = xyz_samples.shape[0]\n",
        "\n",
        "samples = pv.PolyData(xyz_samples)\n",
        "grid_samples = samples.sample(grid)\n",
        "\n",
        "D_samples = np.zeros((2*xyz_samples.shape[0], m//gif_step))\n",
        "for c, i in enumerate(i_plot):\n",
        "    for j in range(m//gif_step):\n",
        "        temp = grid_samples[f'f_{i}_timestep_{j}']\n",
        "        D_samples[c*n_cells_s:(c+1)*n_cells_s,j] = temp\n",
        "\n",
        "sample_res = D_samples_grid.shape\n",
        "fps = 50\n",
        "writergif = PillowWriter(fps=fps)\n",
        "\n",
        "vmins = []\n",
        "vmaxs = []\n",
        "\n",
        "for i in range(len(i_plot)):\n",
        "    vmins.append(np.min(D_samples[i*n_cells_s:(i+1)*n_cells_s, :]))\n",
        "    vmaxs.append(np.max(D_samples[i*n_cells_s:(i+1)*n_cells_s, :]))\n",
        "\n",
        "cmap = 'inferno'\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(3.5, 3.5), dpi=200)\n",
        "levels = 64\n",
        "\n",
        "fig.subplots_adjust(bottom=0., top=1., left=0.275, right=.725, wspace=0.0, hspace=0.05)\n",
        "\n",
        "im1 = axs[0].imshow(D_samples[:n_cells_s,0].reshape(sample_res), cmap=cmap,\n",
        "                vmin=vmins[0], vmax=vmaxs[0], origin='lower')\n",
        "\n",
        "im2 = axs[1].imshow(D_samples[n_cells_s:,0].reshape(sample_res), cmap='viridis',\n",
        "                vmin=vmins[1], vmax=vmaxs[1], origin='lower')\n",
        "\n",
        "ax_bounds1 = axs[0].get_position().bounds\n",
        "cb_ax1 = fig.add_axes([0.2, ax_bounds1[1], 0.025, ax_bounds1[3]])\n",
        "norm = mpl.colors.Normalize(vmin=vmins[0], vmax=vmaxs[0])\n",
        "cbar1 = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
        "                    cax=cb_ax1, orientation='vertical', label=features_pod[i_plot[0]])\n",
        "fmt = mpl.ticker.ScalarFormatter(useMathText=True)\n",
        "cbar1.formatter.set_powerlimits((0, 4))\n",
        "cb_ax1.yaxis.set_offset_position('right')\n",
        "cb_ax1.yaxis.set_ticks_position('left')\n",
        "cb_ax1.yaxis.set_label_position('left')\n",
        "\n",
        "ax_bounds2 = axs[1].get_position().bounds\n",
        "cb_ax2 = fig.add_axes([0.775, ax_bounds2[1], 0.025, ax_bounds2[3]])\n",
        "norm = mpl.colors.Normalize(vmin=vmins[1], vmax=vmaxs[1])\n",
        "cbar2 = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap='viridis'),\n",
        "                    cax=cb_ax2, orientation='vertical', label=features_pod[i_plot[1]])\n",
        "fmt = mpl.ticker.ScalarFormatter(useMathText=True)\n",
        "cbar2.formatter.set_powerlimits((0, 4))\n",
        "cb_ax2.yaxis.set_offset_position('left')\n",
        "\n",
        "for i, ax in enumerate(axs):\n",
        "    if i == 0:\n",
        "        ax.invert_xaxis()\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "    ax.tick_params(axis='x', which='both', bottom=False, labelbottom=False)\n",
        "    ax.tick_params(axis='y', which='both', left=False, labelleft=False)\n",
        "\n",
        "n_frames = m//gif_step\n",
        "anim = FuncAnimation(fig, animate, frames=n_frames, interval=20);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "L4jlq3X1I19H",
        "outputId": "150bae10-488b-4887-c7e2-ca52c892f058"
      },
      "outputs": [],
      "source": [
        "anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJCaecmh7A3q"
      },
      "source": [
        "## Proper Orthogonal Decomposition\n",
        "\n",
        "In general, the goal of data compression algorithms is to define an encoding function $\\mathcal{E}: \\mathbb{R}^{n} ↦ \\mathbb{R}^{q}$ and a decoding function $\\mathcal {D}: \\mathbb{R}^{q} ↦ \\mathbb{R}^{n}$ such that:\n",
        "\n",
        "\\begin{align}\n",
        "  \\tilde{\\mathbf{d}} = \\mathcal{D}(\\mathcal{E}(\\mathbf{d})), \\\\\n",
        "  ||\\tilde{\\mathbf{d}} - \\mathbf{d}||_2^2 < \\epsilon.\n",
        "\\end{align}\n",
        "\n",
        "The array $\\mathbf{d} \\in \\mathbb{R}^n$ represent a full-order snapshot of the combustion system (the temperature and velocity field, for example). Each snapshot has dimensions $n = n_c n_f$ where $n_c$ is the number of computational cells and $n_f$ is the number of features.\n",
        "\n",
        "The encoder compresses the information into a much smaller $q$-dimensional array $\\mathbf{a} = \\mathcal{E}(\\mathbf{d})$. Then, the decoder can be use to reconstruct the full-order snapshot with hopefully minimal error.\n",
        "\n",
        "If we impose that the encoder and the decoder are linear transformations and orthonormal, we obtain:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbf{a} & = \\boldsymbol{\\Phi}^T \\mathbf{d}, \\\\\n",
        "  \\tilde{\\mathbf{d}} & = \\boldsymbol{\\Phi} \\mathbf{a}.\n",
        "\\end{align}\n",
        "\n",
        "From these equations, the reconstructed snapshot can be written as:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\tilde{\\mathbf{d}} = \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T \\mathbf{d}.\n",
        "\\end{equation}\n",
        "\n",
        "The matrix $\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T$ has to be close to the identity matrix if we want to have a low reconstruction error.\n",
        "\n",
        "To find the autoencoding matrix, we need to collect the data and build the data matrix $\\mathbf{D} \\in \\mathbb{R}^{n \\times m}$:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf{D}(\\mathbf{r}, t) = \\begin{bmatrix}\n",
        "        \\vert  & \\vert & & \\vert \\\\\n",
        "        \\mathbf{d}_{1} & \\mathbf{d}_{2} & \\cdots & \\mathbf{d}_{m} \\\\\n",
        "        \\vert  & \\vert & & \\vert \\\\\n",
        "    \\end{bmatrix}  .\n",
        "\\end{equation}\n",
        "\n",
        "Each column of $\\mathbf{D}$ is a snapshot of the system at time $t = i\\Delta t$.\n",
        "\n",
        "The POD objective function can be written as:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\boldsymbol{\\Phi}) & = ||\\mathbf{D} - \\tilde{\\mathbf{D}}||_2^2 =  ||\\mathbf{D} - \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T\\mathbf{D}||_2^2 \\\\\n",
        "\\mathrm{s.t.} & \\ \\ \\ \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} = \\mathbf{I}.\n",
        "\\end{align}\n",
        "\n",
        "The solution of this optimisation problem correspond to the eigenvalue problem:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbf{C} = \\boldsymbol{\\Phi} \\mathbf{L} \\boldsymbol{\\Phi}^T,\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathbf{C} = \\mathbf{D}\\mathbf{D}^T$ is the spatial correlation matrix and $\\mathbf{L}$ is a diagonal matrix containing the eigenvectors of $\\mathbf{C}$.\n",
        "\n",
        "Now we can write the compressed representation of $\\mathbf{D}$ as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{A} = \\boldsymbol{\\Phi}^T \\mathbf{D}.\n",
        "\\end{equation}\n",
        "\n",
        "We can also factorise $\\mathbf{A} = \\boldsymbol{\\Sigma} \\boldsymbol{\\Psi}^T$, where $\\boldsymbol{\\Sigma}$ is a diagonal matrix that contains the $l_2$ norm of the rows of $\\mathbf{A}$.\n",
        "\n",
        "This means that the reconstructed data matrix can be factorised as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tilde{\\mathbf{D}}(\\mathbf{r}, t) = \\boldsymbol{\\Phi}(\\mathbf{r}) \\boldsymbol{\\Sigma} \\boldsymbol{\\Psi}(t)^T.\n",
        "\\end{equation}\n",
        "\n",
        "This equation represents the truncated singular value decomposition of $\\mathbf{D}$. It is worth noting also the fact that the POD decomposes the spatial information (contained in $\\boldsymbol{\\Phi}$) from the temporal information (contained in $\\boldsymbol{\\Psi}$).\n",
        "\n",
        "The matrix $\\boldsymbol{\\Psi}$ contains the temporal coefficients of the POD modes, and it can also be computed from the eigendecomposition of the temporal correlation matrix $\\mathbf{K} = \\mathbf{D}^T\\mathbf{D}$:\n",
        "\n",
        "\\begin{equation}\n",
        " \\mathbf{K} = \\boldsymbol{\\Psi} \\mathbf{L} \\boldsymbol{\\Psi}^T.\n",
        "\\end{equation}\n",
        "\n",
        "So, there are 3 ways to compute the POD decomposition:\n",
        "\n",
        "\n",
        "*   Eigendecomposition of the spatial correlation matrix (classic POD).\n",
        "*   Eigendecomposition of the temporal correlation matrix (snapshot POD).\n",
        "*   Singular value decomposition.\n",
        "\n",
        "### Data preprocessing\n",
        "\n",
        "The POD can only be applied to centered matrices (i.e. matrices in which the mean along the rows is zero). If the matrix is not centered, we can center it by removing the mean:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{D}_0 = \\mathbf{D} - \\mathbf{D}_{\\mu}\n",
        "\\end{equation}\n",
        "\n",
        "Each column of the matrix $\\mathbf{D}_{\\mu}$ contains the temporal mean $\\mathbf{d}_{\\mu}$ computed as:\n",
        "\n",
        "\\begin{equation}\n",
        "  d_{\\mu_i} = \\frac{1}{m}\\sum_{j=1}^{m} d_{i,j} \\ \\ \\ i = 1, \\dots, n.\n",
        "\\end{equation}\n",
        "\n",
        "If the data matrix contains multiple features with different units of measurements or different scales, we also need to normalise the dataset:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf{D}_0 = \\mathbf{D}_{s}^{-1}(\\mathbf{D} - \\mathbf{D}_\\mu).\n",
        "\\end{equation}\n",
        "\n",
        "$\\mathbf{D}_s$ is a $n \\times n $ diagonal matrix computed as:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf{D}_s = \\begin{bmatrix}\n",
        "        s_1 \\mathbf{I} & \\mathbf{0} & \\dots & \\mathbf{0} \\\\\n",
        "        \\mathbf{0} & s_2 \\mathbf{I} & \\dots & \\mathbf{0} \\\\\n",
        "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "        \\mathbf{0} & \\mathbf{0} & \\dots & s_{n_f} \\mathbf{I}\n",
        "    \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathbf{I}$ is the $n_c \\times n_c$ identity matrix. The scaling coefficient $s_i$ is generally chosen to be the standard deviation of the $i$-th feature, so that the variance of each scaled feature is equal to 1.\n",
        "\n",
        "The reconstructed dataset is then:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\tilde{\\mathbf{D}} = \\mathbf{D}_{\\mu} + \\mathbf{D}_{s}(\\boldsymbol{\\Phi}\\boldsymbol{\\Sigma}\\boldsymbol{\\Psi}^T)\n",
        "\\end{equation}\n",
        "\n",
        "or, in vector notation:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\tilde{\\mathbf{d}}(t) = \\mathbf{d}_{\\mu} + \\mathbf{D}_{s}( \\sum_{i=1}^{q} \\boldsymbol{\\phi}_i \\sigma_i \\psi_i(t))\n",
        "\\end{equation}\n",
        "\n",
        "### Truncation\n",
        "\n",
        "We have seen that POD compresses the information into the first $q$ modes, but how do we choose $q$? One way is to select a threshold of the retained variance (or energy) equal to a high percentage of the original variance, such as $90 \\%$, $95 \\%$ or $99 \\%$.\n",
        "\n",
        "The amount of variance retained in each mode is indicated by the respective eigenvalue. The relative variance can be computed as:\n",
        "\n",
        "\\begin{equation}\n",
        "  V(i) = \\frac{l_i}{\\sum_{i=1}^{m} l_i},\n",
        "\\end{equation}\n",
        "\n",
        "while the relative cumulative variance is:\n",
        "\\begin{equation}\n",
        "  V(q) = \\frac{\\sum_{i=1}^{q} l_i}{\\sum_{i=1}^{m} l_i}.\n",
        "\\end{equation}\n",
        "\n",
        "## References:\n",
        "* [Elements of Dimensionality Reduction and Manifold Learning](https://link.springer.com/book/10.1007/978-3-031-10602-6) (book).\n",
        "* [Linear and nonlinear dimensionality reduction from fluid mechanics to machine learning](https://iopscience.iop.org/article/10.1088/1361-6501/acaffe/meta) (paper).\n",
        "* [On the hidden beauty of the proper orthogonal decomposition](https://link.springer.com/article/10.1007/BF00271473) (paper).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qP9xrshgzLOJ"
      },
      "outputs": [],
      "source": [
        "# Exercise 1:\n",
        "# - Compute the eigendecomposition of the temporal correlation matrix\n",
        "# of the D_f10 dataset (considering only the temperature)\n",
        "\n",
        "# - Plot the relative variance of the first 10 eigenvalues\n",
        "# - Plot their cumulative relative energy\n",
        "\n",
        "# Load the dataset and select only the temperature\n",
        "feature = 'T'\n",
        "i_f = features_pod.index(feature)\n",
        "\n",
        "D = np.load('D_pod_f10.npy')[i_f*n_cells:(i_f+1)*n_cells, :]\n",
        "m = D.shape[1]\n",
        "\n",
        "# Tips:\n",
        "# - Use the function print() to print information to the cell's output\n",
        "# - To compute the mean, you can use the function np.mean()\n",
        "# - The arrays can be manipulated using numpy slicing (https://numpy.org/doc/stable/user/basics.indexing.html)\n",
        "# - Matrices can be multiplied using the function np.dot()\n",
        "# - To compute the eigendecomposition, use the function np.linalg.eig()\n",
        "# - To plot, use the function plt.plot() or plt.scatter()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yvf9ImZw5RFk"
      },
      "outputs": [],
      "source": [
        "# Exercise 2:\n",
        "# - Compute the phase angle alpha of the modulating sinusoid.\n",
        "# - Plot the distribution of the temporal coefficients of the first 2 modes,\n",
        "#   colored by the phase angle.\n",
        "\n",
        "# Tips:\n",
        "# - The phase angle is alpha = 2*pi*(time/period % 1) (% is the reminder operator)\n",
        "# - To plot, use plt.scatter(x, y, c=alpha)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "51GWqnHG9OkQ"
      },
      "outputs": [],
      "source": [
        "# Exercise 3:\n",
        "# - Compute the spatial modes\n",
        "# - Plot the 1st and 2nd modes\n",
        "\n",
        "# Tips:\n",
        "# - Compute the l2 norm using np.linalg.norm()\n",
        "# - Use the function plot_snapshot() to plot the modes\n",
        "\n",
        "# Plot them\n",
        "# plot_snapshot(grid, mode1, (0,1,0), (0,0,0), (0,2), '', cmap='inferno')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qjsI3w2nBzZL"
      },
      "outputs": [],
      "source": [
        "# Exercise 4:\n",
        "# - Reconstruct the dataset using only 4 modes\n",
        "# - Compare the original and reconstructed last snapshot\n",
        "\n",
        "# Tips:\n",
        "# - Use the function np.linalg.multi_dot() to multiply 3 or more matrices\n",
        "\n",
        "# plot_snapshot(grid, original, (0,1,0), (0,0,0), (0,2), 'T', cmap='inferno')\n",
        "# plot_snapshot(grid, reconstructed, (0,1,0), (0,0,0), (0,2), 'T', cmap='inferno')\n",
        "# plot_snapshot(grid, difference, (0,1,0), (0,0,0), (0,2), 'T', cmap='inferno')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MKBi9Z9sd4Ra"
      },
      "outputs": [],
      "source": [
        "# Bonus exercise:\n",
        "# - Compute the POD on the matrix containing both the temperature and OH\n",
        "# - Plot the cumulative energy of the eigenvalues\n",
        "# - Plot the first temperature and OH POD modes\n",
        "\n",
        "D = np.load('D_pod_f10.npy')\n",
        "m = D.shape[1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Fd0u2GLuWD"
      },
      "source": [
        "## Principal Component Analysis\n",
        "\n",
        "As a mathematical technique, PCA is exactly the same as POD. However, in the combustion community this technique is called POD when the goal is to analyse the temporal behaviour of dynamical systems, while it is called PCA when it is used to find a low-dimensional projection of the thermo-chemical state (temperature and species concentrations).\n",
        "\n",
        "Finding a low-dimensional representation of the thermo-chemical state is very important in combustion, because reacting simulations can involve hundreds of species and thousands of reactions.\n",
        "\n",
        "In this case, the data matrix $\\mathbf{D} \\in \\mathbb{R}^{s \\times n_f}$ is arranged as:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf{D} = \\begin{bmatrix}\n",
        "        - & \\mathbf{d}_{1} & - \\\\\n",
        "        - & \\mathbf{d}_{2} & -  \\\\\n",
        "          & \\vdots & \\\\\n",
        "        - & \\mathbf{d}_{s} & - \\\\\n",
        "    \\end{bmatrix}  .\n",
        "\\end{equation}\n",
        "\n",
        "Each row represents the position of a point in the thermo-chimical space. By diagonalising the row-wise correlation matrix $\\mathbf{K}$ we obtain the principal components (PCs):\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{K} = \\boldsymbol{\\Psi}\\mathbf{L}\\boldsymbol{\\Psi}^T\n",
        "\\end{equation}\n",
        "\n",
        "The matrix $\\boldsymbol{\\Psi}$ transform the data from its high-dimensional representation $\\mathbf{D}$ to the low-dimensional representation $\\mathbf{Z}$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{Z} = \\mathbf{D} \\boldsymbol{\\Psi}.\n",
        "\\end{equation}\n",
        "\n",
        "The rows of $\\mathbf{Z}$ are called the principal scores.\n",
        "\n",
        "### Data preprocessing\n",
        "\n",
        "As for POD, the dataset needs to be centered and scaled before applying PCA. However, since the data is arranged in a different way, this process is slightly different:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbf{D}_0 = (\\mathbf{D} - \\mathbf{D}_{\\mu})\\mathbf{D}^{-1}_{s}.\n",
        "\\end{equation}\n",
        "\n",
        "The matrix $\\mathbf{D}_{\\mu}$ contains the column-wise average while $\\mathbf{D}_{\\mu} \\in \\mathbb{R}^{n_f \\times n_f}$ is a diagonal matrix containing the standard deviation of each feature.\n",
        "\n",
        "## References:\n",
        "* [Principal component analysis of turbulent combustion data: Data pre-processing and manifold sensitivity](https://doi.org/10.1016/j.combustflame.2012.09.016) (paper).\n",
        "* [Cost function for low-dimensional manifold topology assessment](https://www.nature.com/articles/s41598-022-18655-1) (paper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MuRBaXVABrHN"
      },
      "outputs": [],
      "source": [
        "# Exercise 1:\n",
        "# - Load the file \"D_pca.npy\"\n",
        "# - Plot the O2 vs CO2 concentrations and color them by the temperature\n",
        "#   (plot only 10000 random points)\n",
        "\n",
        "D = np.load('D_pca.npy')\n",
        "features_pca = ['T', 'CH4', 'CO', 'CO2', 'H2', 'H2O', 'N2', 'O2', 'OH']\n",
        "\n",
        "# This is focus on the reactive region\n",
        "i_OH = features_pca.index('OH')\n",
        "mask = D[:, i_OH] > 1e-4\n",
        "D = D[mask, :]\n",
        "\n",
        "# Use random_int to plot the points\n",
        "rng = np.random.default_rng(12312043)\n",
        "random_int = rng.integers(D.shape[0], size=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PSsiir9yk5CX"
      },
      "outputs": [],
      "source": [
        "# Exercise 2:\n",
        "# - Apply PCA to the dataset\n",
        "# - Plot the 1st and 2nd scores and color them by the temperature\n",
        "\n",
        "# Tips:\n",
        "# - Use np.std() to compute the standard deviation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hWgBOg1fmxRK"
      },
      "outputs": [],
      "source": [
        "# Exercise 3:\n",
        "# - Visualise the weights of the first 3 PCs\n",
        "\n",
        "# Tips:\n",
        "# - Use plt.bar() to create a bar plot of the weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4:\n",
        "# - Plot the distribution of the relative variance and cumulative variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bonus: \n",
        "# - Do PCA using PCAFold.\n",
        "# - Select different scaling methods.\n",
        "\n",
        "from PCAfold.reduction import PCA\n",
        "\n",
        "# Tips:\n",
        "# Find the PCA documentation here: https://pcafold.readthedocs.io/en/latest/user/data-reduction.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBPdC5GdpGrY"
      },
      "source": [
        "## Reduced-order model\n",
        "\n",
        "Another important feature of POD is that it allows us to easily build ROMs of combustion systems. A ROM exploits data compression to reduce the complexity of the full-order system and to build a regression model that can instantly predict the system's state.\n",
        "\n",
        "In our example, we want to build a model that predicts the system's state given the phase angle:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbf{d} = f(\\alpha).\n",
        "\\end{equation}\n",
        "\n",
        "The function $f: \\mathbb{R} \\mapsto \\mathbb{R}^{n}$ has very high dimensionality, so it is complex to represent. However, we can first project the data in the low-dimensional manifold, and then build the regression model:\n",
        "\n",
        "\\begin{align}\n",
        "  & a_i = g_i(\\alpha) \\ \\ \\ i=1, \\dots, q \\\\\n",
        "  & \\mathbf{d} = \\sum_{i=1}^{q} \\boldsymbol{\\phi}_i(\\mathbf{r}) \\ a_i(\\alpha).\n",
        "\\end{align}\n",
        "\n",
        "The functions $g_i: \\mathbb{R} \\mapsto \\mathbb{R}$ are much simpler to estimate than $f$.\n",
        "\n",
        "This framework is very similar to the KKL expansion, which is regarded as the generalisation of POD for continuuos functions. This expansion states that a centered stochastic process $D_t$ can be decomposed as:\n",
        "\n",
        "\\begin{equation}\n",
        "  D_t = \\sum_{i=1}^{\\infty} \\Phi_i \\ a_i(t),\n",
        "\\end{equation}\n",
        "\n",
        "where $a_i(t)$ are orthogonal functions.\n",
        "\n",
        "To estimate the functions $g_i$ we can employ different methods from linear regression to neural networks. In our group, we employ the Gaussian Process Regression (GPR), which is a Bayesian method that let us estimate also the uncertainty in the estimation:\n",
        "\n",
        "\\begin{equation}\n",
        "  g_i(\\alpha^*) \\sim \\mathcal{N}(\\bar{g}_i(\\alpha^*), \\mathrm{var}(g_i(\\alpha^*))).\n",
        "\\end{equation}\n",
        "\n",
        "You can find some more information of GPR and the GPR-based ROM in the documentation of [OpenMEASURE](https://github.com/burn-research/OpenMEASURE).\n",
        "\n",
        "## References:\n",
        "* [Application of reduced-order models based on PCA & Kriging for the development of digital twins of reacting flow applications](https://doi.org/10.1016/j.compchemeng.2018.09.022) (paper).\n",
        "* [Parameter Estimation Using a Gaussian Process Regression-Based Reduced-Order Model and Sparse Sensing: Application to a Methane/Air Lifted Jet Flame](https://link.springer.com/article/10.1007/s10494-023-00446-x) (paper).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "U2DKGu9ipEyX"
      },
      "outputs": [],
      "source": [
        "# Exercise 1:\n",
        "# - Compute the spatial modes matrix Phi\n",
        "# - Plot the first 4 functions g_i (the projection of D) as a function of alpha\n",
        "\n",
        "# Load the matrix with only the temperature data\n",
        "# Skip the first 196 timesteps (not periodic)\n",
        "\n",
        "n_init = 196\n",
        "D = np.load('D_pod_fcomplex.npy')[i_f*n_cells:(i_f+1)*n_cells, n_init:]\n",
        "m = D.shape[1]\n",
        "\n",
        "# Tips:\n",
        "# - You can compute Phi using np.svd() with the option full_matrices=False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ymjuCE_gtofy"
      },
      "outputs": [],
      "source": [
        "# Exercise 2:\n",
        "# - Use openmeasure to build a ROM that predicts a from alpha\n",
        "# - Predict the solution and the uncertainty for 100 points between 0-2pi\n",
        "\n",
        "from openmeasure.gpr import GPR\n",
        "\n",
        "# Choose 32 snapshots randomly\n",
        "rng = np.random.default_rng(122213423)\n",
        "i_train = np.arange(m)\n",
        "rng.shuffle(i_train)\n",
        "i_train = i_train[:32]\n",
        "\n",
        "# alpha_train = alpha[i_train, np.newaxis]\n",
        "# D_train = D[:, i_train]\n",
        "\n",
        "xyz = grid.cell_centers().points\n",
        "\n",
        "# Tips:\n",
        "# - Check https://github.com/burn-research/OpenMEASURE/blob/master/docs/gpr_doc.ipynb\n",
        "#   for the instructions on how to use GPR()\n",
        "\n",
        "# - Create the gpr model using the class GPR() and supplying\n",
        "#   the dataset, the number of features, the grid points xyz and alpha\n",
        "\n",
        "# - Compute the POD using gpr.fit() with the option scaleX_type='none'\n",
        "# - Train the model using gpr.train()\n",
        "# - Predict the solution using gpr.predict()\n",
        "# - Use np.linspace() to create 100 equidistant points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rt9fAe0W7Dro"
      },
      "outputs": [],
      "source": [
        "# Exercise 4:\n",
        "# - Plot the predictions a and their uncertainty\n",
        "\n",
        "# Tips:\n",
        "# - Use plt.fill_between() to plot the uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NznXEyqZOcCX"
      },
      "outputs": [],
      "source": [
        "# Exercise 5:\n",
        "# - Reconstruct the dataset D_test from the prediction A_test\n",
        "\n",
        "# Tips:\n",
        "# - Use gpr.reconstruct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "-BTjzwbWiCje",
        "outputId": "7e48f1ca-af2a-4237-942a-dd2dcbd00421"
      },
      "outputs": [],
      "source": [
        "# Bonus: animate the prediction"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
